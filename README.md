# Efficient-Parameter-Fine-tuning-for-LLM/MLLM:Avoiding-Catastrophic-Forgetting


# Sparsity
**Model Tailor: Mitigating Catastrophic Forgetting in
 Multi-modal Large Language Models**
 [link](https://arxiv.org/pdf/2402.12048)
 
Summary: Selecting a subset of neurons to update is a sparsity method.
 
# Optimizer

**MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning**

[link](https://arxiv.org/pdf/2407.20999)

Summary: A modified optimizer, which only updates the momentum Top k (hyperparameter) weights, also seems to be a sparsity approach.

# Averging the weights

**Mitigating the Alignment Tax of RLHF**

[link](https://arxiv.org/pdf/2309.06256)

Summary: Using average weights before and after RLHE seems to work well.
